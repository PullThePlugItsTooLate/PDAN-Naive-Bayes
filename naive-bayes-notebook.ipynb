{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   credit.policy             purpose  int.rate  installment  log.annual.inc  \\\n",
      "0              1  debt_consolidation    0.1189       829.10       11.350407   \n",
      "1              1         credit_card    0.1071       228.22       11.082143   \n",
      "2              1  debt_consolidation    0.1357       366.86       10.373491   \n",
      "3              1  debt_consolidation    0.1008       162.34       11.350407   \n",
      "4              1         credit_card    0.1426       102.92       11.299732   \n",
      "\n",
      "     dti  fico  days.with.cr.line  revol.bal  revol.util  inq.last.6mths  \\\n",
      "0  19.48   737        5639.958333      28854        52.1               0   \n",
      "1  14.29   707        2760.000000      33623        76.7               0   \n",
      "2  11.63   682        4710.000000       3511        25.6               1   \n",
      "3   8.10   712        2699.958333      33667        73.2               1   \n",
      "4  14.97   667        4066.000000       4740        39.5               0   \n",
      "\n",
      "   delinq.2yrs  pub.rec  not.fully.paid  \n",
      "0            0        0               0  \n",
      "1            0        0               0  \n",
      "2            0        0               0  \n",
      "3            0        0               0  \n",
      "4            1        0               0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"loan_data.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not.fully.paid\n",
      "0    8045\n",
      "1    1533\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each class in the 'not.fully.paid' column\n",
    "class_distribution = df['not.fully.paid'].value_counts()\n",
    "\n",
    "# Display the class distribution\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   credit.policy  int.rate  installment  log.annual.inc    dti  fico  \\\n",
      "0              1    0.1189       829.10       11.350407  19.48   737   \n",
      "1              1    0.1071       228.22       11.082143  14.29   707   \n",
      "2              1    0.1357       366.86       10.373491  11.63   682   \n",
      "3              1    0.1008       162.34       11.350407   8.10   712   \n",
      "4              1    0.1426       102.92       11.299732  14.97   667   \n",
      "\n",
      "   days.with.cr.line  revol.bal  revol.util  inq.last.6mths  delinq.2yrs  \\\n",
      "0        5639.958333      28854        52.1               0            0   \n",
      "1        2760.000000      33623        76.7               0            0   \n",
      "2        4710.000000       3511        25.6               1            0   \n",
      "3        2699.958333      33667        73.2               1            0   \n",
      "4        4066.000000       4740        39.5               0            1   \n",
      "\n",
      "   pub.rec  not.fully.paid  purpose_all_other  purpose_credit_card  \\\n",
      "0        0               0              False                False   \n",
      "1        0               0              False                 True   \n",
      "2        0               0              False                False   \n",
      "3        0               0              False                False   \n",
      "4        0               0              False                 True   \n",
      "\n",
      "   purpose_debt_consolidation  purpose_educational  purpose_home_improvement  \\\n",
      "0                        True                False                     False   \n",
      "1                       False                False                     False   \n",
      "2                        True                False                     False   \n",
      "3                        True                False                     False   \n",
      "4                       False                False                     False   \n",
      "\n",
      "   purpose_major_purchase  purpose_small_business  \n",
      "0                   False                   False  \n",
      "1                   False                   False  \n",
      "2                   False                   False  \n",
      "3                   False                   False  \n",
      "4                   False                   False  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'purpose' column to dummy variables\n",
    "df = pd.get_dummies(df, columns=['purpose'])\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Features: (6417, 19) Labels: (6417,)\n",
      "Testing set - Features: (3161, 19) Labels: (3161,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop(columns=['not.fully.paid'])  # Features\n",
    "y = df['not.fully.paid']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (67% train, 33% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set - Features:\", X_train.shape, \"Labels:\", y_train.shape)\n",
    "print(\"Testing set - Features:\", X_test.shape, \"Labels:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the testing set\n",
    "y_pred = nb_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203100284720025\n",
      "F1 Score: 0.13149847094801223\n",
      "Confusion Matrix:\n",
      "[[2550  100]\n",
      " [ 468   43]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Construct confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided output, here's the interpretation:\n",
    "\n",
    "    Accuracy: The accuracy of the model is approximately 82.03%. This indicates that about 82.03% of the predictions made by the model on the test set were correct.\n",
    "\n",
    "    F1 Score: The F1 score is approximately 0.1315. The F1 score is a harmonic mean of precision and recall. A low F1 score indicates that the model has a poor balance between precision and recall.\n",
    "\n",
    "    Confusion Matrix:\n",
    "        True Negative (TN): 2550\n",
    "        False Positive (FP): 100\n",
    "        False Negative (FN): 468\n",
    "        True Positive (TP): 43\n",
    "\n",
    "Interpreting the confusion matrix:\n",
    "\n",
    "    The model correctly predicted 2550 instances of fully paid loans (True Negatives).\n",
    "    It incorrectly predicted 100 instances of not fully paid loans as fully paid (False Positives).\n",
    "    It incorrectly predicted 468 instances of fully paid loans as not fully paid (False Negatives).\n",
    "    It correctly predicted only 43 instances of not fully paid loans (True Positives).\n",
    "\n",
    "Based on these evaluation metrics, we can see that while the accuracy is relatively high, the F1 score is low, indicating that the model may not be performing well in terms of both precision and recall. Additionally, looking at the confusion matrix, we see a relatively high number of false negatives, which suggests that the model is not effectively identifying instances of not fully paid loans.\n",
    "\n",
    "In conclusion, although the accuracy is decent, the F1 score and the imbalance in the confusion matrix suggest that the model's performance may not be satisfactory for predicting not fully paid loans accurately. Further investigation and potentially model improvement are warranted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis step, we interpret the results obtained from evaluating the model and provide insights into its performance. Here are some points to consider for the analysis:\n",
    "\n",
    "    Accuracy:\n",
    "        The accuracy score indicates the proportion of correctly classified instances out of the total instances. In this case, the model achieved an accuracy of approximately 82.03%. While this seems relatively high, it's important to consider the class distribution of the dataset. If the dataset is imbalanced (which we have not explicitly confirmed), accuracy alone may not be a reliable metric.\n",
    "\n",
    "    F1 Score:\n",
    "        The F1 score provides a balance between precision and recall. It considers both false positives and false negatives. A low F1 score suggests that the model's performance in terms of both precision and recall is not satisfactory. In this case, the F1 score is approximately 0.1315, indicating poor performance.\n",
    "\n",
    "    Confusion Matrix:\n",
    "        The confusion matrix provides detailed insights into the model's performance, showing the counts of true positives, true negatives, false positives, and false negatives.\n",
    "        Analyzing the confusion matrix:\n",
    "            The high number of false negatives (468) suggests that the model is failing to correctly identify instances of not fully paid loans, leading to a significant number of misclassifications.\n",
    "            The relatively low number of true positives (43) further indicates the model's difficulty in correctly predicting instances of not fully paid loans.\n",
    "            The presence of false positives (100) indicates instances where the model incorrectly predicts fully paid loans as not fully paid.\n",
    "\n",
    "    Imbalance:\n",
    "        If the dataset is imbalanced, it can affect the model's performance. We have not explicitly checked the balance of the dataset, so it's important to consider whether the observed performance metrics are influenced by class imbalance.\n",
    "\n",
    "    Model Performance:\n",
    "        Based on the evaluation metrics and the analysis of the confusion matrix, we can conclude that the model's performance is not satisfactory for predicting not fully paid loans accurately.\n",
    "        The model exhibits a significant number of false negatives, indicating that it struggles to identify instances of not fully paid loans, which is crucial for the task at hand.\n",
    "\n",
    "    Further Steps:\n",
    "        Further investigation into the dataset, including feature engineering, handling imbalance (if present), and potentially trying other machine learning algorithms or improving the current model, may be necessary to enhance predictive performance.\n",
    "\n",
    "In summary, while the model achieved a decent accuracy, the F1 score and analysis of the confusion matrix reveal weaknesses in its performance, particularly in correctly identifying instances of not fully paid loans. Further steps should be taken to improve the model's performance for this classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
